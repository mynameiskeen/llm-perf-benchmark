# llm_benchmark
This project is used to perform benchmarks on different opensource LLM models on various GPUs and inference engines to find the most efficient solution for self-hosting LLM inference.
The 